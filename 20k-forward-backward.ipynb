{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries \n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms \n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import MNIST\n",
    "# to run on GPU for M1 activate virtual environment with:  conda activate torch-nightly \n",
    "# torch.device(\"mps\") analogous to torch.device(\"cuda\") on an Nvidia GPU.\n",
    "\n",
    "#device = torch.device('mps' if torch.backends.mps.is_built() else 'cpu')\n",
    "device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timing_function_for(Function):\n",
    "    @staticmethod\n",
    "    # ctx is the first argument to forward\n",
    "    def forward(ctx, input, weight, bias=None):\n",
    "        #genrate array of sorted index\n",
    "        input_sorted,sorted_index= torch.sort(input)\n",
    "        sort= torch.zeros_like(sorted_index[0])\n",
    "        \n",
    "        output=torch.zeros(input.shape[0],weight.shape[0])\n",
    "        contri_weight=torch.zeros(input.shape[0],weight.shape[0],weight.shape[1])\n",
    "        for i in range(input_sorted.shape[0]):\n",
    "             sort[sorted_index[i]]=torch.arange(0,input_sorted.shape[1])\n",
    "             #print(input[i],sort)\n",
    "             #generate n new arrays with boolean mask from sorted_indices\n",
    "             weight_broadcast = torch.where(sort.unsqueeze(0) <=torch.arange(0,sort.shape[0]).unsqueeze(1),weight.unsqueeze(1) ,torch.zeros_like(weight).unsqueeze(1))\n",
    "             #print(weight_broadcast)\n",
    "             #change weight_broadcast to float\n",
    "             weight_broadcast = weight_broadcast.float()\n",
    "             #perform matrix multiplication of input and weight_broadcast\n",
    "             all_output = ((Vc*C)+ torch.tensordot(input[i],weight_broadcast, dims=([0],[2])))/weight_broadcast.sum(dim=2)\n",
    "             #print(all_output)\n",
    "             #find the min of all_output\n",
    "             min_values,min_indices = torch.min(all_output, dim=1)\n",
    "            #  print(min_values)\n",
    "            #  print(min_indices)\n",
    "             output[i] = min_values\n",
    "             contri_weight[i] = weight_broadcast[torch.arange(weight_broadcast.size(0)), min_indices].unsqueeze(0)\n",
    " \n",
    "        # The forward pass can use ctx.\n",
    "        ctx.save_for_backward(input, contri_weight, bias, output)\n",
    "        # if bias is not None:\n",
    "        #     output += bias.unsqueeze(0).expand_as(output)\n",
    "        #print(contri_weight)\n",
    "\n",
    "        return output\n",
    " \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, weight, bias, output = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "        # print(input)\n",
    "        # print('weight', weight)\n",
    "        # print('grad_output', grad_output)\n",
    "        # print('output', output)\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = torch.tensordot(grad_output,(weight/weight.sum(dim=2,keepdim=True)), dims=([1],[1]))\n",
    "            #print(grad_input)\n",
    "            grad_input = grad_input[torch.eye(weight.shape[0]).bool()]\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = (input.unsqueeze(1) - output.unsqueeze(2))/weight.sum(dim=2,keepdim=True) \n",
    "            #print(grad_weight)\n",
    "            grad_weight =torch.tensordot(grad_output,(((input.unsqueeze(1) - output.unsqueeze(2))/weight.sum(dim=2,keepdim=True))* (weight !=0).float()), dims=([0],[0]))\n",
    "            grad_weight = grad_weight[torch.eye(weight.shape[1]).bool()]\n",
    "            #print(grad_weight)\n",
    "        # # if bias is not None and ctx.needs_input_grad[2]:\n",
    "        # #     grad_bias = grad_output.sum(0)\n",
    "        # print(weight.sum(dim=2,keepdim=True))\n",
    "        print(grad_input)\n",
    "        print(grad_weight)\n",
    "        return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timing_function(Function):\n",
    "    @staticmethod\n",
    "    # ctx is the first argument to forward\n",
    "    def forward(ctx, input, weight, bias=None):\n",
    "        #genrate array of sorted index\n",
    "        input_sorted,sorted_index= torch.sort(input)\n",
    "        sort= torch.zeros_like(sorted_index)\n",
    "\n",
    "        sort.scatter_(1,sorted_index,torch.arange(0,input_sorted.shape[1]).repeat(input_sorted.shape[0],1))\n",
    "        #print(input,sort)\n",
    "        #generate n new arrays with boolean mask from sorted_indices\n",
    "        weight_broadcast = torch.where(sort.unsqueeze(1).unsqueeze(1) <=torch.arange(0, sort.shape[1]).unsqueeze(0).unsqueeze(2).unsqueeze(1),weight.unsqueeze(1).unsqueeze(0) ,torch.zeros_like(weight).unsqueeze(1).unsqueeze(0))\n",
    "        weight_broadcast= weight_broadcast.float()\n",
    "        # print(\"weight\",weight_broadcast)\n",
    "        #perform matrix multiplication of input and weight_broadcast\n",
    "        diagonal = torch.tensordot(input,weight_broadcast, dims=([1],[3]))\n",
    "        diagonal = diagonal[torch.eye(weight_broadcast.shape[0]).bool()]\n",
    "        all_output =((Vc*C)+diagonal)/weight_broadcast.sum(dim=3)\n",
    "        # print(all_output)\n",
    "        #find the min of all_output\n",
    "        min_values,min_indices = torch.min(all_output, dim=2)\n",
    "        # print(min_values)\n",
    "        #print(min_indices)\n",
    "        output = min_values\n",
    "        contri_weight = weight_broadcast[torch.repeat_interleave(torch.arange(weight_broadcast.size(0)),weight_broadcast.size(1)),torch.arange(weight_broadcast.size(1)).repeat(1,weight_broadcast.size(0)),min_indices.view(-1)].reshape(weight_broadcast.shape[0],weight_broadcast.shape[1],weight_broadcast.shape[2])\n",
    "   \n",
    "        # The forward pass can use ctx.\n",
    "        ctx.save_for_backward(input, contri_weight, bias,output)\n",
    "        # if bias is not None:\n",
    "        #     output += bias.unsqueeze(0).expand_as(output)\n",
    "        return output\n",
    " \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, weight, bias, output = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "        # print(input)\n",
    "        # print('weight', weight)\n",
    "        # print('grad_output', grad_output)\n",
    "        # print('output', output)\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = torch.tensordot(grad_output,(weight/weight.sum(dim=2,keepdim=True)), dims=([1],[1]))\n",
    "            #print(grad_input)\n",
    "            grad_input = grad_input[torch.eye(weight.shape[0]).bool()]\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = (input.unsqueeze(1) - output.unsqueeze(2))/weight.sum(dim=2,keepdim=True) \n",
    "            #print(grad_weight)\n",
    "            grad_weight =torch.tensordot(grad_output,(((input.unsqueeze(1) - output.unsqueeze(2))/weight.sum(dim=2,keepdim=True))* (weight !=0).float()), dims=([0],[0]))\n",
    "            grad_weight = grad_weight[torch.eye(weight.shape[1]).bool()]\n",
    "            #print(grad_weight)\n",
    "        # # if bias is not None and ctx.needs_input_grad[2]:\n",
    "        # #     grad_bias = grad_output.sum(0)\n",
    "        # print(weight.sum(dim=2,keepdim=True))\n",
    "        #print(grad_input)\n",
    "        #print(grad_weight)\n",
    "        return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C and VC can be specified to as parameters to the function or declared within the function\n",
    "C=225\n",
    "Vc=1\n",
    "batch_size=torch.randint(1,10, (1,1)).reshape(1)\n",
    "N=torch.randint(1,10, (1,1)).reshape(1)\n",
    "M=torch.randint(1,10, (1,1)).reshape(1)\n",
    "input=torch.rand(batch_size,M,requires_grad=True)\n",
    "labels= torch.rand(batch_size,N)\n",
    "weight=torch.rand(N,M)\n",
    "weight=weight.float()\n",
    "weight.requires_grad=True\n",
    "\n",
    "output_for=Timing_function_for.apply(input, weight)\n",
    "output_new=Timing_function.apply(input, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of batch tensor([1])\n",
      "number of inputs 4\n",
      "number of neurons 6\n",
      "tensor([[True, True, True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "#verify the output\n",
    "print('size of batch',batch_size)\n",
    "print(\"number of inputs\",input.shape[1])\n",
    "print(\"number of neurons\",weight.shape[0])\n",
    "print(output_for==output_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_for tensor([[120.9888,  71.6021,  53.8713,  59.2155,  93.5532,  44.4865]],\n",
      "       grad_fn=<Timing_function_forBackward>)\n",
      "output_new tensor([[120.9888,  71.6021,  53.8713,  59.2155,  93.5532,  44.4865]],\n",
      "       grad_fn=<Timing_functionBackward>)\n"
     ]
    }
   ],
   "source": [
    "print('output_for',output_for)\n",
    "print('output_new',output_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(6094.2515, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss(reduce='none')\n",
    "loss= criterion(output_new,labels)\n",
    "print('loss',loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "\n",
    "class Downsample(object):\n",
    "    def __init__(self, size=(10, 10)):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Add a batch and channel dimension, resize, then remove batch dimension\n",
    "        return F.interpolate(img.unsqueeze(0), size=self.size, mode='area').squeeze(0)\n",
    "    \n",
    "\n",
    "\n",
    "# Define the transformation pipeline\n",
    "transform_pipeline = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    Downsample(size=(10, 10))\n",
    "])\n",
    "\n",
    "# Load MNIST dataset with the custom transform\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform_pipeline)\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform_pipeline)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=10, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 25 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "figure = plt.figure(figsize=(10, 8))\n",
    "cols, rows = 5, 5\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(mnist_trainset), size=(1,)).item()\n",
    "    img, label = mnist_trainset[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(label)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "img,label=mnist_trainset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 10])"
      ]
     },
     "execution_count": 614,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Temporal(nn.Module):\n",
    "    def __init__(self, input_features,hidden_features, output_features, bias=False):\n",
    "        super().__init__()\n",
    "        self.input_features = input_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.output_features = output_features\n",
    "\n",
    "        # nn.Parameter is a special kind of Tensor, that will get\n",
    "        # automatically registered as Module's parameter once it's assigned\n",
    "        # as an attribute. Parameters and buffers need to be registered, or\n",
    "        # they won't appear in .parameters() (doesn't apply to buffers), and\n",
    "        # won't be converted when e.g. .cuda() is called. You can use\n",
    "        # .register_buffer() to register buffers.\n",
    "        # nn.Parameters require gradients by default.\n",
    "        self.weight1 = nn.Parameter(torch.empty(hidden_features, input_features))\n",
    "        self.weight2 = nn.Parameter(torch.empty(output_features, hidden_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.empty(output_features))\n",
    "        else:\n",
    "            # You should always register all possible parameters, but the\n",
    "            # optional ones can be None if you want.\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        # Not a very smart way to initialize weights\n",
    "        nn.init.normal_(self.weight1, 0.5, 0.005)\n",
    "        nn.init.normal_(self.weight2, 0.5, 0.005)\n",
    "        if self.bias is not None:\n",
    "            nn.init.uniform_(self.bias, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # See the autograd section for explanation of what happens here.\n",
    "        layer1=Timing_function.apply(input, self.weight1, self.bias)\n",
    "        out=Timing_function.apply(layer1, self.weight2, self.bias)\n",
    "        return out\n",
    "\n",
    "    def extra_repr(self):\n",
    "        # (Optional)Set the extra information about this module. You can test\n",
    "        # it by printing an object of this class.\n",
    "        return 'input_features={}, output_features={}, bias={}'.format(\n",
    "            self.input_features, self.output_features, self.bias is not None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal=Temporal(100,100,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal(input_features=100, output_features=10, bias=False)\n"
     ]
    }
   ],
   "source": [
    "print(temporal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset of the first 100 images\n",
    "subset_indices = list(range(60000))\n",
    "mnist_subset = Subset(mnist_trainset, subset_indices)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(mnist_subset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "num_epochs = 100\n",
    "model = Temporal(100,100,10).to(device)\n",
    "\n",
    "# Set Loss function with criterion\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "# Set optimizer with optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "total_step = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 13.9359\n",
      "Accuracy: 7.00%\n",
      "Epoch [2/100], Loss: 10.2038\n",
      "Accuracy: 9.00%\n",
      "Epoch [3/100], Loss: 8.1472\n",
      "Accuracy: 12.00%\n",
      "Epoch [4/100], Loss: 6.8325\n",
      "Accuracy: 10.00%\n",
      "Epoch [5/100], Loss: 5.8750\n",
      "Accuracy: 13.00%\n",
      "Epoch [6/100], Loss: 5.1292\n",
      "Accuracy: 14.00%\n",
      "Epoch [7/100], Loss: 4.5736\n",
      "Accuracy: 12.00%\n",
      "Epoch [8/100], Loss: 4.0910\n",
      "Accuracy: 10.00%\n",
      "Epoch [9/100], Loss: 3.7270\n",
      "Accuracy: 11.00%\n",
      "Epoch [10/100], Loss: 3.4021\n",
      "Accuracy: 11.00%\n",
      "Epoch [11/100], Loss: 3.1398\n",
      "Accuracy: 6.00%\n",
      "Epoch [12/100], Loss: 2.8918\n",
      "Accuracy: 11.00%\n",
      "Epoch [13/100], Loss: 2.6396\n",
      "Accuracy: 14.00%\n",
      "Epoch [14/100], Loss: 2.5032\n",
      "Accuracy: 9.00%\n",
      "Epoch [15/100], Loss: 2.3275\n",
      "Accuracy: 9.00%\n",
      "Epoch [16/100], Loss: 2.1612\n",
      "Accuracy: 11.00%\n",
      "Epoch [17/100], Loss: 2.0123\n",
      "Accuracy: 15.00%\n",
      "Epoch [18/100], Loss: 1.9202\n",
      "Accuracy: 10.00%\n",
      "Epoch [19/100], Loss: 1.7953\n",
      "Accuracy: 11.00%\n",
      "Epoch [20/100], Loss: 1.7108\n",
      "Accuracy: 4.00%\n",
      "Epoch [21/100], Loss: 1.6212\n",
      "Accuracy: 10.00%\n",
      "Epoch [22/100], Loss: 1.5343\n",
      "Accuracy: 13.00%\n",
      "Epoch [23/100], Loss: 1.4505\n",
      "Accuracy: 14.00%\n",
      "Epoch [24/100], Loss: 1.3710\n",
      "Accuracy: 7.00%\n",
      "Epoch [25/100], Loss: 1.3158\n",
      "Accuracy: 6.00%\n",
      "Epoch [26/100], Loss: 1.2593\n",
      "Accuracy: 11.00%\n",
      "Epoch [27/100], Loss: 1.1854\n",
      "Accuracy: 13.00%\n",
      "Epoch [28/100], Loss: 1.1280\n",
      "Accuracy: 12.00%\n",
      "Epoch [29/100], Loss: 1.1050\n",
      "Accuracy: 8.00%\n",
      "Epoch [30/100], Loss: 1.0536\n",
      "Accuracy: 9.00%\n",
      "Epoch [31/100], Loss: 1.0030\n",
      "Accuracy: 15.00%\n",
      "Epoch [32/100], Loss: 0.9543\n",
      "Accuracy: 13.00%\n",
      "Epoch [33/100], Loss: 0.9298\n",
      "Accuracy: 18.00%\n",
      "Epoch [34/100], Loss: 0.8863\n",
      "Accuracy: 8.00%\n",
      "Epoch [35/100], Loss: 0.8591\n",
      "Accuracy: 4.00%\n",
      "Epoch [36/100], Loss: 0.8233\n",
      "Accuracy: 9.00%\n",
      "Epoch [37/100], Loss: 0.7864\n",
      "Accuracy: 11.00%\n",
      "Epoch [38/100], Loss: 0.7699\n",
      "Accuracy: 4.00%\n",
      "Epoch [39/100], Loss: 0.7516\n",
      "Accuracy: 4.00%\n",
      "Epoch [40/100], Loss: 0.7144\n",
      "Accuracy: 16.00%\n",
      "Epoch [41/100], Loss: 0.6868\n",
      "Accuracy: 12.00%\n",
      "Epoch [42/100], Loss: 0.6606\n",
      "Accuracy: 10.00%\n",
      "Epoch [43/100], Loss: 0.6304\n",
      "Accuracy: 14.00%\n",
      "Epoch [44/100], Loss: 0.6306\n",
      "Accuracy: 13.00%\n",
      "Epoch [45/100], Loss: 0.6074\n",
      "Accuracy: 13.00%\n",
      "Epoch [46/100], Loss: 0.5848\n",
      "Accuracy: 13.00%\n",
      "Epoch [47/100], Loss: 0.5631\n",
      "Accuracy: 16.00%\n",
      "Epoch [48/100], Loss: 0.5585\n",
      "Accuracy: 8.00%\n",
      "Epoch [49/100], Loss: 0.5358\n",
      "Accuracy: 20.00%\n",
      "Epoch [50/100], Loss: 0.5075\n",
      "Accuracy: 15.00%\n",
      "Epoch [51/100], Loss: 0.5109\n",
      "Accuracy: 11.00%\n",
      "Epoch [52/100], Loss: 0.4975\n",
      "Accuracy: 9.00%\n",
      "Epoch [53/100], Loss: 0.4804\n",
      "Accuracy: 11.00%\n",
      "Epoch [54/100], Loss: 0.4657\n",
      "Accuracy: 7.00%\n",
      "Epoch [55/100], Loss: 0.4502\n",
      "Accuracy: 12.00%\n",
      "Epoch [56/100], Loss: 0.4346\n",
      "Accuracy: 10.00%\n",
      "Epoch [57/100], Loss: 0.4262\n",
      "Accuracy: 13.00%\n",
      "Epoch [58/100], Loss: 0.4103\n",
      "Accuracy: 17.00%\n",
      "Epoch [59/100], Loss: 0.4078\n",
      "Accuracy: 15.00%\n",
      "Epoch [60/100], Loss: 0.4004\n",
      "Accuracy: 13.00%\n",
      "Epoch [61/100], Loss: 0.3911\n",
      "Accuracy: 11.00%\n",
      "Epoch [62/100], Loss: 0.3799\n",
      "Accuracy: 15.00%\n",
      "Epoch [63/100], Loss: 0.3692\n",
      "Accuracy: 8.00%\n",
      "Epoch [64/100], Loss: 0.3685\n",
      "Accuracy: 11.00%\n",
      "Epoch [65/100], Loss: 0.3515\n",
      "Accuracy: 10.00%\n",
      "Epoch [66/100], Loss: 0.3390\n",
      "Accuracy: 19.00%\n",
      "Epoch [67/100], Loss: 0.3350\n",
      "Accuracy: 6.00%\n",
      "Epoch [68/100], Loss: 0.3301\n",
      "Accuracy: 11.00%\n",
      "Epoch [69/100], Loss: 0.3150\n",
      "Accuracy: 17.00%\n",
      "Epoch [70/100], Loss: 0.3090\n",
      "Accuracy: 9.00%\n",
      "Epoch [71/100], Loss: 0.3071\n",
      "Accuracy: 18.00%\n",
      "Epoch [72/100], Loss: 0.3013\n",
      "Accuracy: 8.00%\n",
      "Epoch [73/100], Loss: 0.3013\n",
      "Accuracy: 9.00%\n",
      "Epoch [74/100], Loss: 0.2899\n",
      "Accuracy: 11.00%\n",
      "Epoch [75/100], Loss: 0.2848\n",
      "Accuracy: 13.00%\n",
      "Epoch [76/100], Loss: 0.2789\n",
      "Accuracy: 12.00%\n",
      "Epoch [77/100], Loss: 0.2812\n",
      "Accuracy: 12.00%\n",
      "Epoch [78/100], Loss: 0.2678\n",
      "Accuracy: 11.00%\n",
      "Epoch [79/100], Loss: 0.2657\n",
      "Accuracy: 6.00%\n",
      "Epoch [80/100], Loss: 0.2620\n",
      "Accuracy: 13.00%\n",
      "Epoch [81/100], Loss: 0.2465\n",
      "Accuracy: 16.00%\n",
      "Epoch [82/100], Loss: 0.2482\n",
      "Accuracy: 10.00%\n",
      "Epoch [83/100], Loss: 0.2456\n",
      "Accuracy: 10.00%\n",
      "Epoch [84/100], Loss: 0.2379\n",
      "Accuracy: 19.00%\n",
      "Epoch [85/100], Loss: 0.2386\n",
      "Accuracy: 10.00%\n",
      "Epoch [86/100], Loss: 0.2312\n",
      "Accuracy: 11.00%\n",
      "Epoch [87/100], Loss: 0.2258\n",
      "Accuracy: 12.00%\n",
      "Epoch [88/100], Loss: 0.2228\n",
      "Accuracy: 18.00%\n",
      "Epoch [89/100], Loss: 0.2204\n",
      "Accuracy: 16.00%\n",
      "Epoch [90/100], Loss: 0.2231\n",
      "Accuracy: 3.00%\n",
      "Epoch [91/100], Loss: 0.2185\n",
      "Accuracy: 10.00%\n",
      "Epoch [92/100], Loss: 0.2151\n",
      "Accuracy: 11.00%\n",
      "Epoch [93/100], Loss: 0.2092\n",
      "Accuracy: 15.00%\n",
      "Epoch [94/100], Loss: 0.2061\n",
      "Accuracy: 12.00%\n",
      "Epoch [95/100], Loss: 0.2049\n",
      "Accuracy: 14.00%\n",
      "Epoch [96/100], Loss: 0.1990\n",
      "Accuracy: 14.00%\n",
      "Epoch [97/100], Loss: 0.1973\n",
      "Accuracy: 11.00%\n",
      "Epoch [98/100], Loss: 0.2027\n",
      "Accuracy: 6.00%\n",
      "Epoch [99/100], Loss: 0.1932\n",
      "Accuracy: 13.00%\n",
      "Epoch [100/100], Loss: 0.1892\n",
      "Accuracy: 15.00%\n"
     ]
    }
   ],
   "source": [
    "#convert a label from 0 to 9 to a one hot vector\n",
    "#convert a batch of labels from 0 to 9 to a batch of one hot vectors\n",
    "def one_hot_batch(labels):\n",
    "    one_hot_labels=torch.zeros(labels.shape[0],10)\n",
    "    for i in range(labels.shape[0]):\n",
    "        one_hot_labels[i][labels[i]]=1\n",
    "    return one_hot_labels\n",
    "#time_scale = (C*Vc)/50\n",
    "time_scale =4\n",
    "# We use the pre-defined number of epochs to determine how many iterations to train the network on\n",
    "for epoch in range(num_epochs):\n",
    "\t#Load in the data in batches using the train_loader object\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        #images = images.type(torch.FloatTensor) # convert the images to float tensors used when running on CPU\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        #convert the labels to one hot vectors and add the time scale to the labels\n",
    "        labels_hot=one_hot_batch(labels)\n",
    "        labels_hot=labels_hot+time_scale\n",
    "        #print(labels_hot)\n",
    "        \n",
    "        # Forward pass\n",
    "        #flatted the image to a vector of size 100\n",
    "        images= images.view(images.size(0), -1)\n",
    "        outputs = model(images) # Pass the images to the CNN model. images contains the batch size of 10 images and the rest of the dimensions are inferred by the CNN\n",
    "        loss = criterion(outputs, labels_hot) # Calculate the loss using the loss function criterion which is the cross entropy loss function\n",
    "\n",
    "        #compute the accuracy of the model\n",
    "        _, predicted = torch.max(outputs.data, 1) # torch.max returns the maximum value and the index of the maximum value in the tensor. The index of the maximum value is the predicted class\n",
    "        #print(\"outputs\",outputs)\n",
    "        #print(\"predicted\",predicted)\n",
    "        #print(\"labels\",labels)\n",
    "        correct = (predicted == labels).sum().item() # sum up the number of correct predictions and convert the tensor to a scalar value\n",
    "        accuracy = correct / labels.size(0) # divide the number of correct predictions by the batch size to get the accuracy of the model\n",
    "        # the difference between accuracy and loss is that accuracy is the number of correct predictions divided by the batch size and loss is the average loss of the batch\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad() # zero out the gradients from the previous iteration\n",
    "        loss.backward() # backpropagate the loss computes the gradients of the loss with respect to the parameters of the model the backward function is called on and take grad_output as the argument which is the gradient of the loss with respect to the output of the model and is computed by the loss function\n",
    "        optimizer.step() # update the parameters of the model using the gradients computed by the backward function\n",
    "\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "    print('Accuracy: {:.2f}%'.format(accuracy*100))\n",
    "\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.3256, 0.3250, 0.3295, 0.3295, 0.3320, 0.3288, 0.3263, 0.3216, 0.3234,\n",
       "         0.3228, 0.3228, 0.3243, 0.3348, 0.3350, 0.3335, 0.3336, 0.3321, 0.3320,\n",
       "         0.3264, 0.3320, 0.3297, 0.3230, 0.3288, 0.3437, 0.3497, 0.3520, 0.3457,\n",
       "         0.3406, 0.3307, 0.3292, 0.3304, 0.3352, 0.3369, 0.3472, 0.3482, 0.3475,\n",
       "         0.3532, 0.3389, 0.3310, 0.3271, 0.3285, 0.3277, 0.3290, 0.3419, 0.3548,\n",
       "         0.3408, 0.3422, 0.3434, 0.3298, 0.3295, 0.3341, 0.3266, 0.3478, 0.3411,\n",
       "         0.3499, 0.3545, 0.3538, 0.3385, 0.3328, 0.3280, 0.3213, 0.3322, 0.3341,\n",
       "         0.3461, 0.3420, 0.3510, 0.3457, 0.3371, 0.3209, 0.3318, 0.3313, 0.3294,\n",
       "         0.3488, 0.3501, 0.3465, 0.3547, 0.3396, 0.3284, 0.3165, 0.3224, 0.3220,\n",
       "         0.3203, 0.3352, 0.3383, 0.3407, 0.3407, 0.3385, 0.3284, 0.3310, 0.3290,\n",
       "         0.3211, 0.3207, 0.3272, 0.3283, 0.3330, 0.3258, 0.3284, 0.3209, 0.3298,\n",
       "         0.3227],\n",
       "        [0.3254, 0.3251, 0.3248, 0.3233, 0.3317, 0.3171, 0.3277, 0.3232, 0.3262,\n",
       "         0.3359, 0.3228, 0.3190, 0.3289, 0.3264, 0.3309, 0.3285, 0.3312, 0.3279,\n",
       "         0.3242, 0.3282, 0.3300, 0.3306, 0.3423, 0.3398, 0.3504, 0.3486, 0.3393,\n",
       "         0.3372, 0.3341, 0.3286, 0.3166, 0.3299, 0.3250, 0.3463, 0.3493, 0.3474,\n",
       "         0.3524, 0.3378, 0.3256, 0.3242, 0.3305, 0.3295, 0.3378, 0.3389, 0.3482,\n",
       "         0.3600, 0.3460, 0.3432, 0.3317, 0.3235, 0.3458, 0.3281, 0.3329, 0.3410,\n",
       "         0.3523, 0.3573, 0.3488, 0.3403, 0.3271, 0.3267, 0.3244, 0.3236, 0.3359,\n",
       "         0.3467, 0.3513, 0.3496, 0.3471, 0.3308, 0.3322, 0.3213, 0.3176, 0.3225,\n",
       "         0.3382, 0.3492, 0.3429, 0.3543, 0.3471, 0.3340, 0.3224, 0.3359, 0.3235,\n",
       "         0.3260, 0.3328, 0.3409, 0.3463, 0.3391, 0.3316, 0.3293, 0.3288, 0.3264,\n",
       "         0.3287, 0.3237, 0.3310, 0.3256, 0.3338, 0.3332, 0.3337, 0.3163, 0.3286,\n",
       "         0.3329],\n",
       "        [0.3204, 0.3288, 0.3244, 0.3348, 0.3247, 0.3247, 0.3277, 0.3253, 0.3174,\n",
       "         0.3295, 0.3229, 0.3198, 0.3238, 0.3353, 0.3425, 0.3362, 0.3241, 0.3195,\n",
       "         0.3380, 0.3284, 0.3274, 0.3302, 0.3309, 0.3389, 0.3390, 0.3539, 0.3476,\n",
       "         0.3312, 0.3293, 0.3213, 0.3254, 0.3312, 0.3345, 0.3436, 0.3492, 0.3541,\n",
       "         0.3502, 0.3383, 0.3324, 0.3294, 0.3225, 0.3235, 0.3288, 0.3401, 0.3489,\n",
       "         0.3472, 0.3536, 0.3321, 0.3188, 0.3227, 0.3272, 0.3274, 0.3382, 0.3482,\n",
       "         0.3541, 0.3600, 0.3440, 0.3393, 0.3359, 0.3278, 0.3212, 0.3312, 0.3331,\n",
       "         0.3476, 0.3555, 0.3570, 0.3482, 0.3382, 0.3262, 0.3191, 0.3305, 0.3388,\n",
       "         0.3399, 0.3516, 0.3554, 0.3445, 0.3451, 0.3334, 0.3311, 0.3281, 0.3287,\n",
       "         0.3214, 0.3304, 0.3343, 0.3397, 0.3393, 0.3438, 0.3258, 0.3301, 0.3243,\n",
       "         0.3314, 0.3334, 0.3274, 0.3242, 0.3247, 0.3271, 0.3238, 0.3262, 0.3261,\n",
       "         0.3273],\n",
       "        [0.3199, 0.3262, 0.3327, 0.3276, 0.3254, 0.3351, 0.3329, 0.3244, 0.3312,\n",
       "         0.3259, 0.3235, 0.3307, 0.3235, 0.3291, 0.3378, 0.3375, 0.3289, 0.3277,\n",
       "         0.3224, 0.3364, 0.3267, 0.3318, 0.3287, 0.3454, 0.3500, 0.3472, 0.3496,\n",
       "         0.3328, 0.3259, 0.3336, 0.3284, 0.3241, 0.3291, 0.3458, 0.3454, 0.3491,\n",
       "         0.3572, 0.3319, 0.3350, 0.3314, 0.3220, 0.3276, 0.3367, 0.3490, 0.3509,\n",
       "         0.3563, 0.3459, 0.3413, 0.3189, 0.3215, 0.3248, 0.3268, 0.3347, 0.3419,\n",
       "         0.3536, 0.3560, 0.3444, 0.3344, 0.3334, 0.3279, 0.3182, 0.3291, 0.3347,\n",
       "         0.3413, 0.3396, 0.3483, 0.3510, 0.3412, 0.3277, 0.3225, 0.3301, 0.3325,\n",
       "         0.3415, 0.3481, 0.3473, 0.3520, 0.3494, 0.3288, 0.3213, 0.3316, 0.3238,\n",
       "         0.3292, 0.3350, 0.3387, 0.3437, 0.3444, 0.3330, 0.3288, 0.3287, 0.3273,\n",
       "         0.3241, 0.3142, 0.3203, 0.3295, 0.3285, 0.3304, 0.3300, 0.3251, 0.3183,\n",
       "         0.3277],\n",
       "        [0.3312, 0.3359, 0.3335, 0.3350, 0.3317, 0.3341, 0.3284, 0.3272, 0.3286,\n",
       "         0.3220, 0.3318, 0.3222, 0.3340, 0.3245, 0.3356, 0.3388, 0.3426, 0.3251,\n",
       "         0.3323, 0.3346, 0.3305, 0.3288, 0.3300, 0.3462, 0.3473, 0.3555, 0.3517,\n",
       "         0.3438, 0.3137, 0.3296, 0.3221, 0.3313, 0.3344, 0.3481, 0.3526, 0.3509,\n",
       "         0.3508, 0.3503, 0.3322, 0.3274, 0.3378, 0.3242, 0.3299, 0.3508, 0.3441,\n",
       "         0.3382, 0.3493, 0.3368, 0.3303, 0.3303, 0.3319, 0.3261, 0.3300, 0.3482,\n",
       "         0.3545, 0.3513, 0.3529, 0.3423, 0.3294, 0.3319, 0.3236, 0.3286, 0.3357,\n",
       "         0.3317, 0.3534, 0.3498, 0.3450, 0.3367, 0.3338, 0.3185, 0.3161, 0.3361,\n",
       "         0.3310, 0.3500, 0.3518, 0.3476, 0.3495, 0.3367, 0.3252, 0.3242, 0.3339,\n",
       "         0.3289, 0.3364, 0.3370, 0.3283, 0.3482, 0.3336, 0.3326, 0.3398, 0.3271,\n",
       "         0.3314, 0.3202, 0.3308, 0.3316, 0.3292, 0.3308, 0.3338, 0.3295, 0.3228,\n",
       "         0.3270],\n",
       "        [0.3217, 0.3290, 0.3314, 0.3306, 0.3311, 0.3209, 0.3323, 0.3272, 0.3241,\n",
       "         0.3304, 0.3308, 0.3208, 0.3342, 0.3234, 0.3359, 0.3424, 0.3367, 0.3319,\n",
       "         0.3244, 0.3232, 0.3343, 0.3285, 0.3223, 0.3389, 0.3512, 0.3584, 0.3555,\n",
       "         0.3341, 0.3276, 0.3249, 0.3289, 0.3243, 0.3365, 0.3444, 0.3541, 0.3440,\n",
       "         0.3498, 0.3387, 0.3248, 0.3173, 0.3237, 0.3220, 0.3432, 0.3499, 0.3519,\n",
       "         0.3595, 0.3500, 0.3376, 0.3421, 0.3232, 0.3262, 0.3247, 0.3296, 0.3479,\n",
       "         0.3476, 0.3556, 0.3559, 0.3328, 0.3268, 0.3234, 0.3302, 0.3286, 0.3342,\n",
       "         0.3582, 0.3479, 0.3564, 0.3477, 0.3404, 0.3304, 0.3260, 0.3259, 0.3336,\n",
       "         0.3422, 0.3340, 0.3472, 0.3589, 0.3481, 0.3414, 0.3220, 0.3298, 0.3327,\n",
       "         0.3276, 0.3397, 0.3356, 0.3521, 0.3433, 0.3374, 0.3376, 0.3337, 0.3195,\n",
       "         0.3367, 0.3307, 0.3359, 0.3234, 0.3272, 0.3309, 0.3299, 0.3294, 0.3255,\n",
       "         0.3266],\n",
       "        [0.3383, 0.3276, 0.3206, 0.3256, 0.3322, 0.3311, 0.3317, 0.3233, 0.3299,\n",
       "         0.3308, 0.3276, 0.3261, 0.3267, 0.3303, 0.3302, 0.3399, 0.3319, 0.3187,\n",
       "         0.3237, 0.3188, 0.3255, 0.3226, 0.3281, 0.3419, 0.3400, 0.3415, 0.3482,\n",
       "         0.3377, 0.3308, 0.3234, 0.3247, 0.3303, 0.3257, 0.3472, 0.3490, 0.3450,\n",
       "         0.3503, 0.3365, 0.3344, 0.3284, 0.3236, 0.3281, 0.3379, 0.3435, 0.3460,\n",
       "         0.3442, 0.3484, 0.3389, 0.3348, 0.3316, 0.3267, 0.3299, 0.3302, 0.3497,\n",
       "         0.3475, 0.3586, 0.3549, 0.3390, 0.3301, 0.3393, 0.3285, 0.3296, 0.3361,\n",
       "         0.3451, 0.3558, 0.3632, 0.3500, 0.3357, 0.3343, 0.3349, 0.3266, 0.3214,\n",
       "         0.3426, 0.3486, 0.3472, 0.3499, 0.3441, 0.3251, 0.3304, 0.3397, 0.3360,\n",
       "         0.3317, 0.3364, 0.3491, 0.3315, 0.3398, 0.3397, 0.3263, 0.3330, 0.3313,\n",
       "         0.3230, 0.3305, 0.3312, 0.3246, 0.3305, 0.3274, 0.3197, 0.3203, 0.3257,\n",
       "         0.3215],\n",
       "        [0.3271, 0.3176, 0.3173, 0.3254, 0.3301, 0.3321, 0.3241, 0.3231, 0.3257,\n",
       "         0.3267, 0.3368, 0.3240, 0.3243, 0.3321, 0.3350, 0.3308, 0.3338, 0.3257,\n",
       "         0.3294, 0.3236, 0.3407, 0.3261, 0.3369, 0.3335, 0.3526, 0.3505, 0.3423,\n",
       "         0.3488, 0.3257, 0.3299, 0.3206, 0.3216, 0.3306, 0.3505, 0.3520, 0.3497,\n",
       "         0.3475, 0.3306, 0.3214, 0.3303, 0.3236, 0.3335, 0.3377, 0.3430, 0.3461,\n",
       "         0.3491, 0.3435, 0.3341, 0.3306, 0.3300, 0.3289, 0.3250, 0.3270, 0.3464,\n",
       "         0.3530, 0.3558, 0.3469, 0.3362, 0.3288, 0.3245, 0.3232, 0.3334, 0.3401,\n",
       "         0.3335, 0.3434, 0.3409, 0.3503, 0.3347, 0.3276, 0.3233, 0.3300, 0.3301,\n",
       "         0.3364, 0.3409, 0.3503, 0.3568, 0.3410, 0.3280, 0.3268, 0.3276, 0.3297,\n",
       "         0.3211, 0.3268, 0.3421, 0.3364, 0.3401, 0.3405, 0.3308, 0.3278, 0.3241,\n",
       "         0.3194, 0.3312, 0.3244, 0.3311, 0.3297, 0.3398, 0.3251, 0.3229, 0.3228,\n",
       "         0.3179],\n",
       "        [0.3262, 0.3323, 0.3296, 0.3296, 0.3287, 0.3340, 0.3319, 0.3286, 0.3258,\n",
       "         0.3398, 0.3354, 0.3242, 0.3231, 0.3338, 0.3347, 0.3281, 0.3252, 0.3307,\n",
       "         0.3238, 0.3286, 0.3312, 0.3307, 0.3287, 0.3386, 0.3438, 0.3571, 0.3529,\n",
       "         0.3385, 0.3160, 0.3378, 0.3285, 0.3202, 0.3260, 0.3424, 0.3404, 0.3479,\n",
       "         0.3464, 0.3395, 0.3308, 0.3249, 0.3312, 0.3276, 0.3338, 0.3438, 0.3455,\n",
       "         0.3454, 0.3535, 0.3363, 0.3232, 0.3293, 0.3179, 0.3189, 0.3327, 0.3489,\n",
       "         0.3537, 0.3530, 0.3496, 0.3380, 0.3256, 0.3283, 0.3237, 0.3277, 0.3382,\n",
       "         0.3472, 0.3521, 0.3462, 0.3486, 0.3442, 0.3209, 0.3306, 0.3352, 0.3240,\n",
       "         0.3364, 0.3459, 0.3374, 0.3632, 0.3413, 0.3263, 0.3252, 0.3287, 0.3327,\n",
       "         0.3208, 0.3330, 0.3407, 0.3442, 0.3412, 0.3365, 0.3235, 0.3298, 0.3317,\n",
       "         0.3293, 0.3270, 0.3298, 0.3244, 0.3300, 0.3267, 0.3210, 0.3224, 0.3209,\n",
       "         0.3217],\n",
       "        [0.3202, 0.3226, 0.3218, 0.3303, 0.3245, 0.3314, 0.3234, 0.3249, 0.3248,\n",
       "         0.3292, 0.3224, 0.3214, 0.3239, 0.3276, 0.3337, 0.3407, 0.3357, 0.3326,\n",
       "         0.3151, 0.3303, 0.3300, 0.3219, 0.3320, 0.3431, 0.3550, 0.3475, 0.3543,\n",
       "         0.3374, 0.3295, 0.3215, 0.3333, 0.3362, 0.3359, 0.3438, 0.3493, 0.3456,\n",
       "         0.3481, 0.3371, 0.3230, 0.3291, 0.3401, 0.3301, 0.3396, 0.3421, 0.3559,\n",
       "         0.3591, 0.3435, 0.3344, 0.3299, 0.3314, 0.3290, 0.3370, 0.3342, 0.3518,\n",
       "         0.3499, 0.3556, 0.3389, 0.3418, 0.3312, 0.3354, 0.3287, 0.3241, 0.3351,\n",
       "         0.3401, 0.3480, 0.3527, 0.3447, 0.3327, 0.3383, 0.3342, 0.3271, 0.3291,\n",
       "         0.3341, 0.3380, 0.3500, 0.3608, 0.3451, 0.3367, 0.3234, 0.3217, 0.3237,\n",
       "         0.3278, 0.3280, 0.3310, 0.3464, 0.3379, 0.3334, 0.3260, 0.3256, 0.3235,\n",
       "         0.3331, 0.3345, 0.3287, 0.3176, 0.3203, 0.3251, 0.3366, 0.3321, 0.3313,\n",
       "         0.3307]], requires_grad=True)"
      ]
     },
     "execution_count": 694,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3383, 0.3276, 0.3206, 0.3256, 0.3322, 0.3311, 0.3317, 0.3233, 0.3299,\n",
      "        0.3308, 0.3276, 0.3261, 0.3267, 0.3303, 0.3302, 0.3399, 0.3319, 0.3187,\n",
      "        0.3237, 0.3188, 0.3255, 0.3226, 0.3281, 0.3419, 0.3400, 0.3415, 0.3482,\n",
      "        0.3377, 0.3308, 0.3234, 0.3247, 0.3303, 0.3257, 0.3472, 0.3490, 0.3450,\n",
      "        0.3503, 0.3365, 0.3344, 0.3284, 0.3236, 0.3281, 0.3379, 0.3435, 0.3460,\n",
      "        0.3442, 0.3484, 0.3389, 0.3348, 0.3316, 0.3267, 0.3299, 0.3302, 0.3497,\n",
      "        0.3475, 0.3586, 0.3549, 0.3390, 0.3301, 0.3393, 0.3285, 0.3296, 0.3361,\n",
      "        0.3451, 0.3558, 0.3632, 0.3500, 0.3357, 0.3343, 0.3349, 0.3266, 0.3214,\n",
      "        0.3426, 0.3486, 0.3472, 0.3499, 0.3441, 0.3251, 0.3304, 0.3397, 0.3360,\n",
      "        0.3317, 0.3364, 0.3491, 0.3315, 0.3398, 0.3397, 0.3263, 0.3330, 0.3313,\n",
      "        0.3230, 0.3305, 0.3312, 0.3246, 0.3305, 0.3274, 0.3197, 0.3203, 0.3257,\n",
      "        0.3215], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#extract model weights\n",
    "weight=model.weight1\n",
    "print(model.weight1[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 4., 4., 4., 4., 4., 4., 4., 4., 4.],\n",
       "        [4., 5., 4., 4., 4., 4., 4., 4., 4., 4.],\n",
       "        [4., 4., 5., 4., 4., 4., 4., 4., 4., 4.],\n",
       "        [4., 4., 4., 5., 4., 4., 4., 4., 4., 4.],\n",
       "        [4., 4., 4., 4., 5., 4., 4., 4., 4., 4.],\n",
       "        [4., 4., 4., 4., 4., 5., 4., 4., 4., 4.],\n",
       "        [4., 4., 4., 4., 4., 4., 5., 4., 4., 4.],\n",
       "        [4., 4., 4., 4., 4., 4., 4., 5., 4., 4.],\n",
       "        [4., 4., 4., 4., 4., 4., 4., 4., 5., 4.],\n",
       "        [4., 4., 4., 4., 4., 4., 4., 4., 4., 5.]])"
      ]
     },
     "execution_count": 719,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inital_weight= nn.init.normal_(torch.empty(100, 10), 0.5, 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Linear)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
